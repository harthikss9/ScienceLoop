{
  "simulation_equations": [
    "iter(i) = iteration_number_when_node_i_is_removed_in_k_shell_decomposition",
    "p(i) = 1 / (1 + exp(-sqrt(iter(i))))",
    "PN_p(i) = ks_i + sum_{j in Gamma(i)} p(j)",
    "PN_n(i) = sum_{j in Gamma(i)} sum_{l in Gamma(j)} k_l",
    "r_{1j} = PN_p(j) / sum_{j=1}^n PN_p(j)",
    "r_{2j} = PN_n(j) / sum_{j=1}^n PN_n(j)",
    "H_1 = -(1/ln(n)) * sum_{j=1}^n r_{1j} * ln(r_{1j})",
    "H_2 = -(1/ln(n)) * sum_{j=1}^n r_{2j} * ln(r_{2j})",
    "w_1 = (1 - H_1) / (2 - (H_1 + H_2))",
    "w_2 = (1 - H_2) / (2 - (H_1 + H_2))",
    "PN(i) = w_1 * PN_p(i) + w_2 * PN_n(i)",
    "SIR_model: dS/dt = -beta * S * I / N, dI/dt = beta * S * I / N - gamma * I, dR/dt = gamma * I",
    "F(i) = sum_{t=1}^T I_i(t) where I_i(t) is number of infected nodes at time t when node i is initial spreader",
    "tau = (n_c - n_d) / (n(n-1)/2) where n_c is concordant pairs, n_d is discordant pairs"
  ],
  "constants_required": [
    {
      "name": "beta",
      "description": "Infection rate in SIR model",
      "value_or_range": "[0.05, 0.15] typical range, paper uses multiple values"
    },
    {
      "name": "gamma",
      "description": "Recovery rate in SIR model",
      "value_or_range": "0.1 (fixed, as per typical SIR model setup)"
    },
    {
      "name": "T_max",
      "description": "Maximum time steps for SIR simulation",
      "value_or_range": "100-200 time steps"
    },
    {
      "name": "num_simulations",
      "description": "Number of Monte Carlo runs for each initial spreader",
      "value_or_range": "100-1000 runs for statistical reliability"
    }
  ],
  "variables_to_vary": [
    {
      "name": "network_topology",
      "description": "Type of complex network to test",
      "range": "['scale_free_BA', 'small_world_WS', 'random_ER', 'real_world_networks']",
      "units": "categorical"
    },
    {
      "name": "network_size",
      "description": "Number of nodes in the network",
      "range": "[100, 500, 1000, 2000]",
      "units": "nodes"
    },
    {
      "name": "network_parameters",
      "description": "Specific parameters for network generation (e.g., m for BA, k and p for WS)",
      "range": "BA: m=[2,3,4,5], WS: k=[4,6,8], p=[0.1,0.3,0.5], ER: p=[0.01,0.05,0.1]",
      "units": "dimensionless"
    },
    {
      "name": "weighting_scheme",
      "description": "Method for combining position and neighbor attributes",
      "range": "['entropy_weighted', 'equal_weighted', 'position_only', 'neighbor_only']",
      "units": "categorical"
    },
    {
      "name": "beta_sir",
      "description": "Infection rate parameter for SIR model",
      "range": "[0.05, 0.08, 0.10, 0.12, 0.15]",
      "units": "rate"
    }
  ],
  "procedure_steps": [
    "Step 1: Initialize simulation environment - Import networkx, numpy, scipy for network generation and analysis. Set random seeds for reproducibility.",
    "Step 2: Generate network topologies - Create multiple network instances for each topology type (BA scale-free, WS small-world, ER random) with varying parameters. Store adjacency matrices and degree sequences.",
    "Step 3: Perform k-shell decomposition - For each network, implement k-shell decomposition algorithm tracking iter(i) for each node (iteration number when node is removed). Compute ks_i (shell value) for all nodes.",
    "Step 4: Calculate position attribute PN_p(i) - Apply sigmoid function p(i) = 1/(1 + exp(-sqrt(iter(i)))) to iteration values. Compute PN_p(i) = ks_i + sum of p(j) over all neighbors j of node i.",
    "Step 5: Calculate neighbor attribute PN_n(i) - For each node i, sum the degrees of all second-order neighbors: PN_n(i) = sum over neighbors j of (sum of degrees k_l of neighbors l of j).",
    "Step 6: Compute information entropy weights - Normalize PN_p and PN_n values to get r_{1j} and r_{2j}. Calculate entropies H_1 and H_2 using formula H_i = -(1/ln(n)) * sum(r_{ij} * ln(r_{ij})). Derive weights w_1 = (1-H_1)/(2-(H_1+H_2)) and w_2 = (1-H_2)/(2-(H_1+H_2)).",
    "Step 7: Generate node rankings for different weighting schemes - Entropy-weighted: PN(i) = w_1*PN_p(i) + w_2*PN_n(i). Equal-weighted: PN(i) = 0.5*PN_p(i) + 0.5*PN_n(i). Position-only: PN(i) = PN_p(i). Neighbor-only: PN(i) = PN_n(i). Rank nodes in descending order for each scheme.",
    "Step 8: Run SIR simulations - For each node i as initial spreader, run SIR model with parameters beta and gamma. Use discrete-time stochastic SIR: at each time step, infected nodes infect susceptible neighbors with probability beta, and recover with probability gamma. Run multiple Monte Carlo simulations (100-1000 runs) for each initial spreader.",
    "Step 9: Calculate propagation capability F(i) - For each node i, compute F(i) as the average final outbreak size (total number of recovered nodes at end of simulation) across all Monte Carlo runs when i is the initial spreader. Rank nodes by F(i) in descending order to get ground truth ranking.",
    "Step 10: Compute Kendall's tau correlation - For each weighting scheme, calculate Kendall's tau coefficient between the predicted ranking (from PN(i)) and the ground truth ranking (from F(i)). Kendall's tau = (n_c - n_d) / (n(n-1)/2) where n_c is concordant pairs and n_d is discordant pairs.",
    "Step 11: Repeat for parameter variations - Iterate steps 2-10 for different network topologies, sizes, parameters, and beta values. Store all Kendall's tau values in structured arrays indexed by (topology, size, parameters, weighting_scheme, beta).",
    "Step 12: Statistical analysis - For each combination of network type and parameters, compute mean and standard deviation of Kendall's tau across multiple network realizations. Perform statistical tests (e.g., paired t-test) to determine if entropy-weighted method significantly outperforms other methods.",
    "Step 13: Generate visualizations - Create bar plots comparing Kendall's tau for different weighting schemes across network types. Generate heatmaps showing performance variation with network parameters. Plot Kendall's tau vs beta for each method. Create scatter plots of predicted rank vs actual rank for visual assessment of ranking quality.",
    "Step 14: Validate hypothesis - Compare entropy-weighted method against baselines. Hypothesis is supported if entropy-weighted consistently achieves higher Kendall's tau (statistically significant) across diverse network topologies and parameters."
  ],
  "expected_outcomes": "The simulation should reveal that the entropy-weighted method (w_1*PN_p + w_2*PN_n with information entropy-derived weights) produces node influence rankings with significantly higher Kendall's tau correlation to SIR propagation outcomes compared to equal weighting (0.5*PN_p + 0.5*PN_n), position-only (PN_p), and neighbor-only (PN_n) methods. Expected patterns: (1) Kendall's tau for entropy-weighted method should be 0.05-0.15 points higher than equal-weighted across most networks, (2) Performance advantage should be most pronounced in heterogeneous networks (scale-free BA) compared to more homogeneous networks (random ER), (3) Single-attribute methods (position-only, neighbor-only) should perform worst, demonstrating the value of combining both attributes, (4) The entropy weighting should adaptively adjust w_1 and w_2 based on network structure, with w_1 typically higher in networks where position matters more (hierarchical structures) and w_2 higher in densely connected networks, (5) Performance should be robust across different beta values, though absolute Kendall's tau may vary with infection rate, (6) Visualization should show tighter clustering of predicted vs actual ranks for entropy-weighted method compared to baselines, (7) Statistical tests should confirm significance (p < 0.05) of performance improvements across multiple network realizations."
}