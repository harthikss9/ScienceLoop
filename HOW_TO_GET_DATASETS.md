# How to Get Datasets - For New Users

## Problem
When you clone/pull the repository, the `datasets/` folders are empty or don't exist because **datasets are generated dynamically**, not stored in git.

## Solution: Run DatasetAgent

**Datasets are NOT stored in git** - they must be generated by running `DatasetAgent` for each paper.

### Step-by-Step Instructions

1. **Make sure you have dependencies installed:**
   ```bash
   pip install -r requirements.txt
   ```

2. **For each paper, run DatasetAgent BEFORE CodeGeneratorAgent:**

   **Example for saish paper:**
   ```bash
   python agents/DatasetAgent.py --plan "papers/saish/simulation_plan.json"
   ```
   
   This will:
   - Analyze the simulation plan
   - Determine what datasets are needed
   - Generate/download the datasets
   - Save them to `papers/saish/datasets/`
   - Create `papers/saish/datasets_manifest.json`

3. **Then run CodeGeneratorAgent:**
   ```bash
   python agents/CodeGeneratorAgent.py --plan "papers/saish/simulation_plan.json" --datasets "papers/saish/datasets_manifest.json"
   ```

## How DatasetAgent Works

DatasetAgent analyzes the `simulation_plan.json` and:

1. **Determines dataset type** using Claude 4.5 Sonnet:
   - `graph` - Network/graph datasets (Barabási–Albert, Watts–Strogatz, etc.)
   - `ml` - Machine learning datasets (MNIST, etc.)
   - `bio_structures` - PDB protein structure files
   - `synthetic` - Synthetic data (peptides, classification data, time series)
   - `none` - No datasets needed (pure computational simulations)

2. **Generates/downloads datasets:**
   - **Synthetic datasets**: Generated using numpy (random data, peptides, classification data)
   - **Graph datasets**: Generated using networkx (scale-free, small-world, random graphs)
   - **PDB files**: Downloaded from RCSB Protein Data Bank
   - **ML datasets**: Placeholders (full download requires torch/tensorflow)

3. **Saves datasets** to `papers/{paper_name}/datasets/` subdirectory

4. **Creates manifest** file `datasets_manifest.json` with paths to all datasets

## Complete Pipeline Order

**IMPORTANT:** Run agents in this order:

```bash
# Phase 1: Paper Analysis
python agents/PaperUnderstandingAgent.py --pdf "papers/saish/saish paper.pdf"
python agents/KnowledgeGraphAgent.py --input "papers/saish/understand.json"
python agents/HypothesisAgent.py --input "papers/saish/understand.json" --kg "papers/saish/knowledge_graph.json"
python agents/SimulationPlanAgent.py --hypothesis "papers/saish/hypothesis.json" --paper "papers/saish/understand.json" --kg "papers/saish/knowledge_graph.json"

# Phase 2: Code & Data Preparation
python agents/DatasetAgent.py --plan "papers/saish/simulation_plan.json"  # ← GENERATES DATASETS
python agents/CodeGeneratorAgent.py --plan "papers/saish/simulation_plan.json" --datasets "papers/saish/datasets_manifest.json"
```

## Why Datasets Aren't in Git

- Datasets can be large (1000s of samples)
- They're generated deterministically (same plan = same datasets)
- Reduces repository size
- Allows customization (different parameters = different datasets)

## Troubleshooting

**If DatasetAgent fails:**
- Check that `simulation_plan.json` exists
- Ensure dependencies are installed: `numpy`, `networkx`, `pandas`, `requests`
- Check `.env` file has `ANTHROPIC_API_KEY` set (for Claude)

**If datasets folder is empty:**
- Check `datasets_manifest.json` - if `dataset_type: "none"`, no datasets are needed
- Check DatasetAgent output for errors
- Verify simulation plan mentions data requirements

## Example Output

After running DatasetAgent, you should see:
```
papers/saish/
├── datasets/
│   └── synthetic/
│       └── classification_data.csv
└── datasets_manifest.json
```

The `datasets_manifest.json` will contain:
```json
{
  "dataset_type": "synthetic",
  "datasets": {
    "classification_data": "papers/saish/datasets/synthetic/classification_data.csv"
  }
}
```

